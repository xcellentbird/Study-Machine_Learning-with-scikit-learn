{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning: PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORZS+WoPJUOO2teS1gUwQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xcellentbird/Machine_Learning/blob/main/Deep_Learning_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIgGGl3fHlJC"
      },
      "source": [
        "## PyTorch: Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzQ9j_0_HfIH"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKmSAmb5HrlF"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "w1 = torch.randn(D_in, H, device=device)\n",
        "w2 = torch.randn(H, D_out, device=device)\n",
        "\n",
        "lr = 1e-6\n",
        "epochs = 500"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDR1qxQ1IFTl"
      },
      "source": [
        "for t in range(epochs):\n",
        "  h = x.mm(w1) # layer1. 행렬곱. size = 64 * 100.\n",
        "  h_relu = h.clamp(min=0) # relu. clamp:최대 또는 최소값으로 깎는 함수. size = 64 * 100.\n",
        "  y_pred = h_relu.mm(w2) # layer2. 행렬곱. size = 64 * 10\n",
        "  loss = (y_pred - y).pow(2).sum() # loss 계산. (예측값 - 타겟값)^2의 합\n",
        "\n",
        "  grad_y_pred = 2.0 * (y_pred - y) # loss 미분값. 2 * (예측값 - 타겟값). size = 64 * 10\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred) # [100 * 64] * [64 * 10] = 100 * 10\n",
        "  \n",
        "  grad_h_relu = grad_y_pred.mm(w2.t()) # [64 * 10] * [10 * 100] = 64 * 100\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0 # h 요소 중에서 음수값에 해당하는 index를 가져와 grad_h에서 해당 index의 값을 0으로 설정한다.\n",
        "  grad_w1 = x.t().mm(grad_h) # [1000 * 64] * [64 * 100] = 1000 * 100\n",
        "\n",
        "  w1 -= lr * grad_w1\n",
        "  w2 -= lr * grad_w2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "905UkstZWpvE"
      },
      "source": [
        "## PyTorch: Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucd58AgEPnZN"
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "w1 = torch.randn(D_in, H, device=device, requires_grad=True) # 학습이 필요한 가중치이므로 requires_grad= Ture 설정하여 미분 그래프를 그리도록 유도\n",
        "w2 = torch.randn(H, D_out, device=device, requires_grad=True) # ...\n",
        "\n",
        "lr = 1e-6\n",
        "epochs = 500"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkNyPC4AW17T"
      },
      "source": [
        "for t in range(epochs):\n",
        "  # forward(). 중간 값들을 수동으로 계산할 필요가 없다. \n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "\n",
        "  # backward()를 통해 미분값(grad)을 계산한다.\n",
        "  loss.backward()\n",
        "\n",
        "  # gradient를 추적할 필요없다는 것을 명시해두고, 가중치를 update한다.\n",
        "  with torch.no_grad():\n",
        "    w1 -= lr * w1.grad\n",
        "    w2 -= lr * w2.grad\n",
        "    # 가중치를 update했으니 다음 학습을 위해 grad를 0으로 초기화 시켜준다.\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPOPJWIYpKe"
      },
      "source": [
        "## PyTorch: New Autograd Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr0g3ZB7XLT0"
      },
      "source": [
        "class MyReLU(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x):\n",
        "    ctx.save_for_backward(x)\n",
        "    return x.clamp(min=0)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_y):\n",
        "    x, = ctx.saved_tensors\n",
        "    grad_input = grad_y.clone()\n",
        "    grad_input[x < 0] = 0\n",
        "    return grad_input"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL1lRrwiZHqX"
      },
      "source": [
        "def my_relu(x):\n",
        "  return MyReLU.apply(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SUIpbhdZoZH"
      },
      "source": [
        "## PyTorch: nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn4NGF5lZmg2",
        "outputId": "adcb8621-6cc9-4134-f767-ed3338b25554"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out))\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c1owhQHZ1p4"
      },
      "source": [
        "lr = 1e-6\n",
        "epochs = 500\n",
        "for t in range(epochs):\n",
        "  y_pred = model(x)\n",
        "  loss = torch.nn.functional.mse_loss(y_pred, y)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param -= lr * param.grad\n",
        "  model.zero_grad()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QAGENmYazLf"
      },
      "source": [
        "## PyTorch: optim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq8zlAtLaK3b"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "lr = 1e-6\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxHgyrcIbAAi"
      },
      "source": [
        "epochs = 500\n",
        "for t in range(epochs):\n",
        "  y_pred = model(x)\n",
        "  loss = torch.nn.functional.mse_loss(y_pred, y)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChXulTYYhkzC"
      },
      "source": [
        "## PyTorch: nn Define new Modules\n",
        "\n",
        "Sequential은 단순히 층층이 쌓는 구조였다면, Module은 좀 더 복잡하게 모델을 구성할 수 있게 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mU8ZsTbHqE"
      },
      "source": [
        "class CustomNet(torch.nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    super(CustomNet, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(D_in, H)\n",
        "    self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_relu = self.linear1(x).clamp(min=0)\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-LduSI4jXnm"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)\n",
        "\n",
        "model = CustomNet(D_in, H, D_out)\n",
        "model.to(device)\n",
        "\n",
        "lr = 1e-6\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ufAeh3yqYbH",
        "outputId": "13cff093-2068-40a8-e983-f043599e288d"
      },
      "source": [
        "model"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomNet(\n",
              "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
              "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV0So7Nhjep8"
      },
      "source": [
        "epochs = 500\n",
        "for t in range(epochs):\n",
        "  y_pred = model(x)\n",
        "  loss = torch.nn.functional.mse_loss(y_pred, y)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuvgZhBerBs3"
      },
      "source": [
        "## PyTorch: DataLoaders\n",
        "dataloader를 사용하여 minibatch, shuffling, multithreading 등 다양한 방법으로 데이터를 가져올 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AOzauVsj5Ud"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device=device)\n",
        "y = torch.randn(N, D_out, device=device)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7NDK-QrMA0"
      },
      "source": [
        "loader = DataLoader(TensorDataset(x, y), batch_size=8)\n",
        "model = CustomNet(D_in, H, D_out)\n",
        "model.to(device)\n",
        "\n",
        "lr = 1e-6\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY_I50JergCH"
      },
      "source": [
        "epochs = 20\n",
        "for epoch in range(20):\n",
        "  for x_batch, y_batch in loader:\n",
        "    y_pred = model(x_batch)\n",
        "    loss = torch.nn.functional.mse_loss(y_pred, y_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOixtY7YtZqY"
      },
      "source": [
        "## PyTorch: Dynamic Computation Graphs\n",
        ": 연산과 그래프를 쌓는 과정을 동시에 진행시키는 것을 Dynamic Computation Graph라고 합니다. 연산이 끝나고 그래프를 모두 없애기 때문에 비효율적일 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuihJ8Jeu0PP"
      },
      "source": [
        "## PyTorch: Static Computation Graphs\n",
        ": Dynamic과 다르게 그래프를 쌓고 연산을 진행합니다. 연산이 끝나고 그래프를 없애지 않고 남겨두어 다음 eopch 때 재사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKS34KvtryId"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}